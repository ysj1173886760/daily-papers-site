<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation - Daily AI Papers</title>
    <link rel="stylesheet" href="/assets/style.css">
    <link rel="stylesheet" href="/assets/highlight.css">
    <link rel="alternate" type="application/rss+xml" title="Daily AI Papers" href="/rss.xml">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fafafa;
        }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        header p {
            margin: 0.5rem 0 0 0;
            opacity: 0.9;
        }
        nav {
            margin-top: 1rem;
        }
        nav a {
            color: white;
            text-decoration: none;
            margin: 0 1rem;
            padding: 0.5rem 1rem;
            border-radius: 5px;
            background: rgba(255,255,255,0.2);
        }
        nav a:hover {
            background: rgba(255,255,255,0.3);
        }
        .paper-card {
            background: white;
            border-radius: 10px;
            padding: 2rem;
            margin: 2rem 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-left: 4px solid #667eea;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9rem;
            margin-bottom: 1rem;
        }
        .paper-title {
            font-size: 1.4rem;
            font-weight: bold;
            color: #333;
            margin-bottom: 1rem;
        }
        .paper-summary {
            color: #444;
            line-height: 1.8;
            font-size: 1rem;
        }
        .paper-summary h1 {
            color: #667eea;
            font-size: 1.4rem;
            margin: 2rem 0 1rem 0;
            padding: 0.8rem 1rem;
            background: linear-gradient(90deg, #f8f9ff 0%, #ffffff 100%);
            border-left: 4px solid #667eea;
            border-radius: 0 6px 6px 0;
        }
        .paper-summary h2 {
            color: #5a67d8;
            font-size: 1.2rem;
            margin: 1.5rem 0 0.8rem 0;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid #e2e8f0;
        }
        .paper-summary h3 {
            color: #4a5568;
            font-size: 1.1rem;
            margin: 1.2rem 0 0.6rem 0;
        }
        .paper-summary p {
            margin: 1rem 0;
            text-align: justify;
        }
        .paper-summary ul, .paper-summary ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        .paper-summary li {
            margin: 0.5rem 0;
            line-height: 1.6;
        }
        .paper-summary strong {
            color: #2d3748;
            font-weight: 600;
        }
        .paper-summary em {
            color: #4a5568;
            font-style: italic;
        }
        .paper-summary blockquote {
            margin: 1.5rem 0;
            padding: 1rem 1.5rem;
            background: #f7fafc;
            border-left: 4px solid #667eea;
            border-radius: 0 6px 6px 0;
        }
        .paper-summary code {
            background: #f1f5f9;
            color: #475569;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Fira Code', 'Monaco', 'Consolas', 'Ubuntu Mono', monospace;
            font-size: 0.9rem;
            border: 1px solid #e2e8f0;
        }
        .paper-summary pre {
            background: #0f172a;
            color: #f8fafc;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            border: 1px solid #334155;
            position: relative;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        .paper-summary pre::before {
            content: 'Python';
            position: absolute;
            top: 0;
            right: 0;
            background: #667eea;
            color: white;
            padding: 0.25rem 0.75rem;
            font-size: 0.75rem;
            border-radius: 0 8px 0 8px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
        }
        .paper-summary pre code {
            background: none;
            color: inherit;
            padding: 0;
            border: none;
            font-size: 0.95rem;
            line-height: 1.6;
        }
        .paper-summary .codehilite {
            margin: 1.5rem 0;
        }
        .paper-summary .codehilite pre {
            margin: 0;
        }
        /* 语法高亮颜色 */
        .paper-summary .codehilite .c { color: #6b7280; } /* 注释 */
        .paper-summary .codehilite .k { color: #8b5cf6; } /* 关键字 */
        .paper-summary .codehilite .s { color: #10b981; } /* 字符串 */
        .paper-summary .codehilite .n { color: #f8fafc; } /* 变量名 */
        .paper-summary .codehilite .o { color: #f59e0b; } /* 操作符 */
        .paper-summary .codehilite .p { color: #94a3b8; } /* 标点 */
        .paper-summary .codehilite .m { color: #ef4444; } /* 数字 */
        .paper-summary .codehilite .nf { color: #06b6d4; } /* 函数名 */
        .paper-links {
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid #eee;
        }
        .paper-links a {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 0.5rem 1rem;
            text-decoration: none;
            border-radius: 5px;
            margin-right: 1rem;
            font-size: 0.9rem;
        }
        .paper-links a:hover {
            background: #5a6fd8;
        }
        footer {
            text-align: center;
            margin-top: 3rem;
            padding: 2rem;
            color: #666;
            border-top: 1px solid #eee;
        }
    </style>
</head>
<body>
    <header>
        <h1>Daily AI Papers</h1>
        <p>2025年08月03日 - RAG_test 论文</p>
        <nav>
            <a href="/">首页</a>
            <a href="/rss.xml">RSS订阅</a>
            <a href="/about.html">关于</a>
        </nav>
    </header>

    <main>
        <article class="paper-card">
            <div class="paper-meta">
                <span>发布: 2025-07-31</span> | 
                <span>更新: 2025-07-31</span> |
                <span>分类: cs.LG</span> |
                <span>arXiv ID: 2507.23217v1</span>
            </div>
            
            <h2 class="paper-title">Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation</h2>
            
            <div class="paper-summary">
                <h1 id="_1">问题定义</h1>
<p><strong>problem</strong></p>
<ol>
<li>论文要解决的具体问题是<strong>复杂多模态文档的理解与检索挑战</strong>，具体包括：<ul>
<li>非结构化文档（如商业报告、技术手册）缺乏清晰结构，现有固定大小 chunk 分割破坏语义连贯性，导致检索结果碎片化；</li>
<li>多模态内容（文本、图像、表格、图表）处理需要专门工具或训练，现有方法难以统一处理；</li>
<li>传统文档检索系统依赖训练数据或格式 cues（如标题、元数据），无法适应 diverse 文档类型，且计算复杂度高（O(N)）。</li>
</ul>
</li>
<li>这个问题的重要性体现在：<ul>
<li>现实世界中，大部分文档（如扫描件、无固定格式的报告）是非结构化的，且包含多模态内容，其理解是法律、医疗、金融等领域的核心需求；</li>
<li>现有方法（如固定 chunk 分割、依赖OCR的 pipeline）无法有效保留语义结构或处理多模态，导致检索效率低、答案不准确（如 hallucination）；</li>
<li>训练-free 解决方案对资源有限的场景（如小公司、多语言文档）至关重要，可避免数据收集和模型微调的成本。</li>
</ul>
</li>
<li>目前存在的挑战或困难：<ul>
<li>文档结构提取：非结构化文档缺乏明确的标题或元数据，传统方法（如LayoutLM）需要训练，无法零样本适应；</li>
<li>多模态统一表示：图像、表格等内容的语义需要转化为文本，但现有方法（如OCR）依赖格式或容易丢失结构信息；</li>
<li>检索效率与准确性平衡：固定 chunk 分割导致检索范围大（O(N)），而分层检索需要有效的结构信息，现有方法缺乏这种结构。</li>
</ul>
</li>
</ol>
<h1 id="_2">研究背景</h1>
<p><strong>background</strong></p>
<ol>
<li>前人在这个领域的研究成果包括：<ul>
<li><strong>文档视觉问答（DocVQA）</strong>：早期方法假设单页包含证据，近期 benchmark（如SlideVQA、ICDAR 2023）要求跨页推理，但现有模型依赖OCR或固定 chunk 分割；</li>
<li><strong>检索增强生成（RAG）</strong>：结合检索与生成，如RAVQA、REVEAL，但传统RAG用固定 chunk 分割，破坏语义；</li>
<li><strong>文档结构提取</strong>：LumberChunker用LLM分割小说为章节，提高RAG性能，但未处理多模态或分层检索；</li>
<li><strong>OCR-based vs OCR-free</strong>：OCR-based（如LayoutLM）依赖文本提取，OCR-free（如Donut、Kosmos-1）直接处理图像，但后者对复杂布局处理有限；</li>
<li><strong>大 multimodal 模型</strong>：BLIP-2、PaLI、Kosmos-1等能处理多模态，但文档检索系统未充分利用其语义理解能力。</li>
</ul>
</li>
<li>现有方法的优点和局限性：<ul>
<li><strong>优点</strong>：RAG结合检索与生成，提高答案准确性；OCR-free避免OCR质量依赖；大 multimodal 模型能处理多种内容类型。</li>
<li><strong>局限性</strong>：固定 chunk 分割破坏语义连贯性；需要训练数据（如LayoutLM），无法零样本适应；多模态处理依赖专门工具（如表格提取、图像描述），未统一到LLM框架；检索效率低（O(N)）。</li>
</ul>
</li>
<li>当前技术水平：<ul>
<li>多模态大模型（如GPT-4o、Gemini-1.5）能处理文本、图像、表格等内容，但文档检索系统仍用传统 pipeline（OCR→分割→检索），未充分利用LLM的语义理解；</li>
<li>文档结构提取依赖格式 cues（如HTML书签）或训练数据，无法处理非结构化文档；</li>
<li>RAG系统的检索效率与准确性平衡未解决，固定 chunk 分割导致检索结果碎片化。</li>
</ul>
</li>
</ol>
<h1 id="_3">创新来源</h1>
<p><strong>idea_source</strong></p>
<ol>
<li>核心思路来自<strong>现有技术的 synergistic 整合</strong>：将伪TOC生成（语义结构化）、零样本多模态分析（LLM原生能力）、分层检索（效率优化）结合，形成训练-free的端到端系统。</li>
<li>灵感来自对现有方法局限性的观察：<ul>
<li>固定 chunk 分割破坏语义，因此用LLM生成伪TOC来结构化文档，保留语义连贯性；</li>
<li>传统RAG效率低，因此用分层检索（粗搜 sections→细搜 chunks）降低复杂度；</li>
<li>多模态处理需要专门工具，因此用LLM的原生 multimodal 能力（如视觉分析、表格理解），无需额外模型。</li>
</ul>
</li>
<li>作者产生创新想法的过程：<ul>
<li>首先，意识到文档结构是检索的关键，而非固定 chunk 分割；</li>
<li>然后，发现LLM能通过 prompt 生成伪TOC（边界检测、标题生成），无需训练；</li>
<li>最后，将伪TOC与分层检索结合，利用结构信息优化检索流程，同时用双嵌入提高检索准确性。</li>
</ul>
</li>
</ol>
<h1 id="_4">解决方案</h1>
<p><strong>solution</strong></p>
<ol>
<li>具体技术方案是<strong>DocsRay</strong>，整合三个核心组件：<ul>
<li><strong>伪TOC生成</strong>：用LLM通过 prompt 生成 hierarchical 结构，包括边界检测（判断相邻页是否为新 topic）和标题生成（总结 section 内容）；</li>
<li><strong>零样本多模态分析</strong>：用LLM处理文本、图像、表格等内容，将非文本内容（如表格、图像）转化为文本描述，统一表示；</li>
<li><strong>分层检索</strong>：粗搜（匹配 query 与 section 的标题/内容嵌入）→细搜（在 top sections 内检索 chunks），降低复杂度。</li>
</ul>
</li>
<li>关键技术细节：<ul>
<li><strong>伪TOC生成</strong>：</li>
<li>边界检测：用 prompt 让LLM判断相邻页是否为新 topic（输出0/1），基于语义而非格式；</li>
<li>标题生成：用 prompt 让LLM总结 section 内容，生成 concise 标题；</li>
<li>算法流程：初始分割→大小约束合并→标题生成（见核心算法部分）。</li>
<li><strong>双嵌入 architecture</strong>：</li>
<li>选择BGE-M3（关键词检索）和Multilingual-E5-Large（语义理解），concatenation 后 L2 归一化，保留两者的语义信息；</li>
<li>实验显示，双嵌入比单模型提高8-9个百分点（见表6）。</li>
<li><strong>分层检索</strong>：</li>
<li>粗搜：计算 query 与 section 的标题嵌入（etitle）、内容嵌入（econtent）的相似度，加权融合（β=0.3）；</li>
<li>细搜：在 top-k sections 内检索 chunks，降低搜索范围；</li>
<li>复杂度从O(N)降低到O(S + k1·Ns)（S为 sections 数，Ns为 section 内 chunks 数）。</li>
<li><strong>多模态处理</strong>：</li>
<li>表格：转化为图像，用LLM分析结构与内容；</li>
<li>图像：用 prompt 生成描述（如“解释图表数据”“描述照片内容”）；</li>
<li>多列文本：用空间聚类恢复阅读顺序。</li>
</ul>
</li>
</ol>
<h1 id="_5">实验验证</h1>
<p><strong>experiment</strong></p>
<ol>
<li>实验设计：<ul>
<li><strong>基准</strong>：用MMLongBench-Doc（多页、多模态文档理解 benchmark），评估准确性；</li>
<li><strong>模型变体</strong>：DocsRay-Pro（27B）、DocsRay-Base（12B）、DocsRay-Lite（4B），均用Gemma-3 family，无需训练；</li>
<li>** ablation 研究**：验证伪TOC生成（有无伪TOC）、双嵌入（单模型 vs 双模型）的影响；</li>
<li><strong>定性分析</strong>：分析证据 grounding（单源/多源证据、缺失信息）、模型 scaling（不同大小模型的性能差异）。</li>
</ul>
</li>
<li>数据集与评估指标：<ul>
<li><strong>数据集</strong>：MMLongBench-Doc，包含多页文档（平均49.4页、20971 tokens），涵盖文本、图像、表格等内容；</li>
<li><strong>指标</strong>：准确性（Accuracy）、查询延迟（Query Latency）、证据 grounding（是否引用正确 sections/chunks）。</li>
</ul>
</li>
<li>实验结果：<ul>
<li><strong>准确性</strong>：DocsRay-Pro在MMLongBench-Doc上达到64.7%，超过现有基线（如GPT-4.1的49.7%、Gemini-1.5-Pro的31.2%），接近人类专家（65.8%）；</li>
<li><strong>效率</strong>：伪TOC生成使查询延迟从3.89秒降低到2.12秒（45% improvement）；</li>
<li><strong>ablation 结果</strong>：</li>
<li>伪TOC生成：有伪TOC的模型（62.8%）比无伪TOC的模型（63.5%）准确性略低，但延迟降低45%，体现效率与准确性的平衡；</li>
<li>双嵌入：concatenation 后的双嵌入（62.8%）比单模型（BGE-M3的54.0%、E5-Large的54.7%）提高8-9个百分点；</li>
<li><strong>定性分析</strong>：</li>
<li>单源事实检索：所有模型都能正确检索（如“Gestalt psychology 诞生地”）；</li>
<li>多页证据合成：Pro模型能正确聚合多页信息（如“计数 human quotes”），而小模型（Lite）会 overcount；</li>
<li>证据归因：Pro模型能正确识别缺失信息（如“2024年数据未提供”），避免 hallucination。</li>
</ul>
</li>
</ol>
<h1 id="_6">研究结论</h1>
<p><strong>conclusion</strong></p>
<ol>
<li>重要结论：<ul>
<li>DocsRay通过整合伪TOC生成、分层检索、零样本多模态分析，实现了训练-free的复杂文档理解，性能接近人类专家；</li>
<li>伪TOC生成是关键：通过LLM生成的 hierarchical 结构，保留了语义连贯性，优化了检索流程；</li>
<li>分层检索有效降低了计算复杂度（从O(N)到O(S + k1·Ns)），同时保持了检索准确性；</li>
<li>双嵌入（BGE-M3 + E5-Large）提高了检索准确性，比单模型更能捕捉语义信息。</li>
</ul>
</li>
<li>主要贡献：<ul>
<li><strong>方法创新</strong>：提出训练-free的文档理解系统，整合伪TOC生成、分层RAG、零样本多模态分析；</li>
<li><strong>技术贡献</strong>：</li>
<li>伪TOC生成算法：用两个 prompt 实现边界检测和标题生成，无需训练；</li>
<li>双嵌入 architecture：concatenation 两个互补嵌入模型，提高检索准确性；</li>
<li>分层检索流程：粗搜 sections→细搜 chunks，降低复杂度；</li>
<li><strong>性能贡献</strong>：在MMLongBench-Doc上达到 state-of-the-art 性能（64.7%），接近人类专家。</li>
</ul>
</li>
<li>对领域发展的意义：<ul>
<li>为文档理解提供了<strong>训练-free的解决方案</strong>，可快速部署到 diverse 文档类型（如商业报告、技术手册），避免数据收集和模型微调的成本；</li>
<li>证明了<strong>结构信息</strong>（伪TOC）对检索的重要性，而非固定 chunk 分割；</li>
<li>展示了LLM的<strong>原生 multimodal 能力</strong>（如视觉分析、表格理解）可替代专门工具，简化 pipeline。</li>
</ul>
</li>
</ol>
<h1 id="_7">未来展望</h1>
<p><strong>future_work</strong></p>
<ol>
<li>当前工作的局限性：<ul>
<li><strong>依赖LLM选择</strong>：伪TOC生成的质量依赖 backbone LLM（如Gemini-1.5 Pro），不同LLM可能产生不同的分割结果；</li>
<li><strong>多图像处理</strong>：对需要跨图像比较的任务（如SlideVQA）性能低（Pro的EM为17.1%），因文本描述无法保留像素级信息；</li>
<li><strong>多语言验证</strong>：仅在英语文档上评估，未验证多语言（如中文、阿拉伯语）的性能；</li>
<li><strong>证据 grounding 粒度</strong>：目前仅支持 section 级引用，未实现 sentence 或 pixel 级的细粒度 grounding。</li>
</ul>
</li>
<li>未来可能的改进方向：<ul>
<li><strong>多语言文档处理</strong>：开发多语言 benchmark（涵盖20+语言），验证双嵌入（BGE-M3、E5-Large）在非英语文档上的性能；</li>
<li><strong>嵌入融合策略</strong>：探索更复杂的融合方法（如 attention-based fusion、learned projections），提高双嵌入的准确性；</li>
<li><strong>细粒度证据 grounding</strong>：实现 sentence 级或 pixel 级的引用，提高答案的可验证性；</li>
<li><strong>复杂布局处理</strong>：结合 layout 信息（如文本 bounding box），提高对复杂布局文档（如表单、流程图）的理解。</li>
</ul>
</li>
<li>值得深入探索的问题：<ul>
<li><strong>伪TOC生成的鲁棒性</strong>：如何让伪TOC生成适应不同文档类型（如法律文档、医疗记录），减少对 prompt 的依赖；</li>
<li><strong>多模态检索的统一表示</strong>：如何将图像、表格等内容的语义信息更有效地融入检索嵌入，提高跨模态检索准确性；</li>
<li><strong>训练-free的泛化能力</strong>：如何让DocsRay适应未见过的文档类型（如手写文档、古籍），无需调整 prompt 或模型。</li>
</ul>
</li>
</ol>
<h1 id="_8">核心算法</h1>
<p>pseudocode: |</p>
<div class="codehilite"><pre><span></span><code>  <span class="c1">## 伪TOC生成算法（Algorithm 1）</span>
  <span class="n">输入</span><span class="err">：</span><span class="n">文档页</span> <span class="n">P</span> <span class="o">=</span> <span class="p">{</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">pn</span><span class="p">}</span><span class="err">；</span><span class="n">参数</span><span class="err">：</span><span class="n">初始</span> <span class="n">chunk</span> <span class="n">大小</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="err">，</span><span class="n">最小页数</span> <span class="n">m</span><span class="o">=</span><span class="mi">3</span><span class="err">，</span><span class="n">最大页数</span> <span class="n">M</span><span class="o">=</span><span class="mi">15</span>
  <span class="n">输出</span><span class="err">：</span><span class="n">Sections</span> <span class="n">S</span> <span class="o">=</span> <span class="p">{</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">sj</span><span class="p">}</span>

  <span class="c1">### Phase 1: 初始分割</span>
  <span class="mf">1.</span> <span class="n">将文档分为大小为</span> <span class="n">k</span> <span class="n">的</span> <span class="n">chunks</span><span class="err">；</span>
  <span class="mf">2.</span> <span class="n">初始化</span> <span class="n">boundaries</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="err">；</span>
  <span class="mf">3.</span> <span class="n">对每个</span> <span class="n">chunk</span> <span class="n">i</span><span class="err">（</span><span class="n">从1到chunks</span><span class="o">-</span><span class="mi">1</span><span class="err">）：</span>
     <span class="n">a</span><span class="o">.</span> <span class="n">提取</span> <span class="n">chunk</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span> <span class="n">的最后500字符</span><span class="err">（</span><span class="n">end_text</span><span class="err">）</span><span class="n">和</span> <span class="n">chunk</span> <span class="n">i</span> <span class="n">的前500字符</span><span class="err">（</span><span class="n">start_text</span><span class="err">）；</span>
     <span class="n">b</span><span class="o">.</span> <span class="n">用</span> <span class="n">prompt</span> <span class="n">让LLM判断是否为新</span> <span class="n">topic</span><span class="err">（</span><span class="n">输出0</span><span class="o">/</span><span class="mi">1</span><span class="err">）；</span>
     <span class="n">c</span><span class="o">.</span> <span class="n">如果是新</span> <span class="n">topic</span><span class="err">（</span><span class="n">isnewtopic</span><span class="o">=</span><span class="mi">1</span><span class="err">），</span><span class="n">将</span> <span class="n">i</span><span class="err">×</span><span class="n">k</span> <span class="n">添加到</span> <span class="n">boundaries</span><span class="err">；</span>
  <span class="mf">4.</span> <span class="n">根据</span> <span class="n">boundaries</span> <span class="n">将文档分为初始</span> <span class="n">sections</span> <span class="n">S</span><span class="s1">&#39;。</span>

  <span class="c1">### Phase 2: 大小约束合并</span>
  <span class="mf">1.</span> <span class="n">对每个</span> <span class="n">section</span> <span class="n">si</span> <span class="err">∈</span> <span class="n">S</span><span class="s1">&#39;：</span>
     <span class="n">a</span><span class="o">.</span> <span class="n">如果si</span> <span class="o">&lt;</span> <span class="n">m</span><span class="err">（</span><span class="n">太小</span><span class="err">），</span><span class="n">计算</span> <span class="n">si</span> <span class="n">与相邻</span> <span class="n">sections</span><span class="err">（</span><span class="n">si</span><span class="o">-</span><span class="mi">1</span><span class="err">、</span><span class="n">si</span><span class="o">+</span><span class="mi">1</span><span class="err">）</span><span class="n">的内容嵌入相似度</span><span class="err">；</span>
     <span class="n">b</span><span class="o">.</span> <span class="n">将</span> <span class="n">si</span> <span class="n">合并到相似度更高的相邻</span> <span class="n">section</span><span class="err">（</span><span class="n">simprev</span> <span class="o">&gt;</span> <span class="n">simnext</span> <span class="n">则合并到</span> <span class="n">si</span><span class="o">-</span><span class="mi">1</span><span class="err">，</span><span class="n">否则合并到</span> <span class="n">si</span><span class="o">+</span><span class="mi">1</span><span class="err">）；</span>
  <span class="mf">2.</span> <span class="n">得到合并后的</span> <span class="n">sections</span> <span class="n">S</span><span class="s1">&#39;&#39;</span><span class="err">。</span>

  <span class="c1">### Phase 3: 标题生成</span>
  <span class="mf">1.</span> <span class="n">对每个</span> <span class="n">section</span> <span class="n">s</span> <span class="err">∈</span> <span class="n">S</span><span class="s1">&#39;&#39;</span><span class="err">：</span>
     <span class="n">a</span><span class="o">.</span> <span class="n">采样</span> <span class="n">section</span> <span class="n">的代表性内容</span><span class="err">（</span><span class="n">如开头段落</span><span class="err">）；</span>
     <span class="n">b</span><span class="o">.</span> <span class="n">用</span> <span class="n">prompt</span> <span class="n">让LLM生成</span> <span class="n">concise</span> <span class="n">标题</span><span class="err">；</span>
     <span class="n">c</span><span class="o">.</span> <span class="n">将标题赋值给</span> <span class="n">s</span><span class="o">.</span><span class="n">title</span><span class="err">；</span>
  <span class="mf">2.</span> <span class="n">输出最终</span> <span class="n">sections</span> <span class="n">S</span><span class="err">。</span>

  <span class="c1">## 分层检索流程</span>
  <span class="n">输入</span><span class="err">：</span><span class="n">query</span> <span class="n">q</span><span class="err">；</span><span class="n">文档</span> <span class="n">sections</span> <span class="n">S</span> <span class="o">=</span> <span class="p">{</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">sj</span><span class="p">}</span><span class="err">；</span><span class="n">每个</span> <span class="n">section</span> <span class="n">的</span> <span class="n">chunks</span> <span class="n">Cs</span>
  <span class="n">输出</span><span class="err">：</span><span class="n">top</span><span class="o">-</span><span class="n">k</span> <span class="n">chunks</span>

  <span class="c1">### 1. 粗搜（Section 级）</span>
  <span class="n">a</span><span class="o">.</span> <span class="n">计算</span> <span class="n">query</span> <span class="n">嵌入</span> <span class="n">eq</span> <span class="o">=</span> <span class="n">DualEmbed</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="err">；</span>
  <span class="n">b</span><span class="o">.</span> <span class="n">对每个</span> <span class="n">section</span> <span class="n">s</span><span class="err">：</span>
     <span class="n">i</span><span class="o">.</span> <span class="n">计算标题嵌入</span> <span class="n">etitle</span> <span class="o">=</span> <span class="n">DualEmbed</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">title</span><span class="p">)</span><span class="err">；</span>
     <span class="n">ii</span><span class="o">.</span> <span class="n">计算内容嵌入</span> <span class="n">econtent</span> <span class="o">=</span> <span class="n">平均所有</span> <span class="n">chunk</span> <span class="n">嵌入</span><span class="err">；</span>
     <span class="n">iii</span><span class="o">.</span> <span class="n">计算相似度</span> <span class="n">ssection</span> <span class="o">=</span> <span class="n">β·cos</span><span class="p">(</span><span class="n">eq</span><span class="p">,</span> <span class="n">etitle</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">β</span><span class="p">)</span><span class="err">·</span><span class="n">cos</span><span class="p">(</span><span class="n">eq</span><span class="p">,</span> <span class="n">econtent</span><span class="p">)</span><span class="err">（</span><span class="n">β</span><span class="o">=</span><span class="mf">0.3</span><span class="err">）；</span>
  <span class="n">c</span><span class="o">.</span> <span class="n">选择</span> <span class="n">top</span><span class="o">-</span><span class="n">k1</span> <span class="n">sections</span><span class="err">（</span><span class="n">如k1</span><span class="o">=</span><span class="mi">5</span><span class="err">）。</span>

  <span class="c1">### 2. 细搜（Chunk 级）</span>
  <span class="n">a</span><span class="o">.</span> <span class="n">对</span> <span class="n">top</span><span class="o">-</span><span class="n">k1</span> <span class="n">sections</span> <span class="n">中的每个</span> <span class="n">chunk</span> <span class="n">c</span><span class="err">：</span>
     <span class="n">i</span><span class="o">.</span> <span class="n">计算</span> <span class="n">chunk</span> <span class="n">嵌入</span> <span class="n">ec</span> <span class="o">=</span> <span class="n">DualEmbed</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="err">；</span>
     <span class="n">ii</span><span class="o">.</span> <span class="n">计算相似度</span> <span class="n">schunk</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">eq</span><span class="p">,</span> <span class="n">ec</span><span class="p">)</span><span class="err">；</span>
  <span class="n">b</span><span class="o">.</span> <span class="n">选择</span> <span class="n">top</span><span class="o">-</span><span class="n">k2</span> <span class="n">chunks</span><span class="err">（</span><span class="n">如k2</span><span class="o">=</span><span class="mi">10</span><span class="err">）。</span>

  <span class="c1">### 3. 结果返回</span>
  <span class="n">返回</span> <span class="n">top</span><span class="o">-</span><span class="n">k2</span> <span class="n">chunks</span><span class="err">，</span><span class="n">并附上</span> <span class="n">section</span> <span class="n">级引用</span><span class="err">。</span>

  <span class="c1">## 双嵌入生成</span>
  <span class="n">输入</span><span class="err">：</span><span class="n">文本</span> <span class="n">t</span><span class="err">；</span><span class="n">模型1</span><span class="err">（</span><span class="n">BGE</span><span class="o">-</span><span class="n">M3</span><span class="err">）；</span><span class="n">模型2</span><span class="err">（</span><span class="n">Multilingual</span><span class="o">-</span><span class="n">E5</span><span class="o">-</span><span class="n">Large</span><span class="err">）</span>
  <span class="n">输出</span><span class="err">：</span><span class="n">合并嵌入</span> <span class="n">ecombined</span>

  <span class="mf">1.</span> <span class="n">用模型1生成嵌入</span> <span class="n">e1</span> <span class="o">=</span> <span class="n">BGE</span><span class="o">-</span><span class="n">M3</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="err">；</span>
  <span class="mf">2.</span> <span class="n">用模型2生成嵌入</span> <span class="n">e2</span> <span class="o">=</span> <span class="n">Multilingual</span><span class="o">-</span><span class="n">E5</span><span class="o">-</span><span class="n">Large</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="err">；</span>
  <span class="mf">3.</span> <span class="n">合并嵌入</span><span class="err">：</span><span class="n">ecombined</span> <span class="o">=</span> <span class="n">L2_normalize</span><span class="p">(</span><span class="n">concatenate</span><span class="p">(</span><span class="n">e1</span><span class="p">,</span> <span class="n">e2</span><span class="p">))</span><span class="err">；</span>
  <span class="mf">4.</span> <span class="n">返回</span> <span class="n">ecombined</span><span class="err">。</span>
</code></pre></div>
            </div>
            
            <div class="paper-links">
                <a href="http://arxiv.org/abs/2507.23217" target="_blank">arXiv原文</a>
                <a href="http://arxiv.org/pdf/2507.23217" target="_blank">PDF下载</a>
            </div>
        </article>
    </main>

    <footer>
        <p>Generated by Daily Paper Processing System | Template: V2</p>
        <p>数据来源: arXiv | 分析引擎: Large Language Model</p>
    </footer>
</body>
</html>